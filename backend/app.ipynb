{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89afa4dea654d429d154941a625a65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Archit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Archit\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7b3a4c284843a78006330b05069141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41529cc0c074dc9a326ea2fb961c441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2569d84fdf664c739a8c119a3256d01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e6cbc3343345b388e528f316ea131e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e006679df84dc68fc1686b3373454e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea5fde1c3ca4b098af011d53c6df10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "model=GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(text):\n",
    "    encoded_input= tokenizer.encode(text, add_special_tokens=False, return_tensors='pt')\n",
    "    input_ids=encoded_input[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs=model(input_ids)\n",
    "        logits=outputs.logits\n",
    "\n",
    "\n",
    "    perplexity = torch.exp(torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), input_ids.view(-1)))\n",
    "\n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_burstiness(text):\n",
    "    tokens=nltk.word_tokenize(text.lower())\n",
    "    word_freq=FreqDist(tokens)\n",
    "\n",
    "    repeated_count=sum(count>1 for count in word_freq.values())\n",
    "\n",
    "    burstiness_score=repeated_count/len(word_freq)\n",
    "\n",
    "    return burstiness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16968.384765625\n",
      "0.3150684931506849\n",
      "human content\n"
     ]
    }
   ],
   "source": [
    "text=\"We are building a community for content writers where they can showcase their work and writings without any need to prove their authenticity. Our platform will help content writers showcase their work by providing a badge of trust that highlights that their content is plagiarism-free and has not been copied from some other place. We will provide the content writers with a certificate to attach to their work after they get their content scanned by us and it passes the authenticity check. We are also providing a marketplace where content writers can list down their content which is approved by us and get the opportunity to meet business owners that will no longer have to worry about the writers being honest.\"\n",
    "\n",
    "p=calculate_perplexity(text)\n",
    "b=calculate_burstiness(text)\n",
    "\n",
    "print(p)\n",
    "print(b)\n",
    "\n",
    "if(p>30000 and b<0.2):\n",
    "    print(\"Ai generated content\")\n",
    "else:\n",
    "    print('human content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
